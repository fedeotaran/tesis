\subsection{Unificación y recolección}
\label{unificacion_y_recoleccion}

El siguiente paso es tomar los datos de ambas fuentes, unificarlos y enviarlos
a nuestra instancia en ejecución de \gls{term:elasticsearch}. Para cumplir con
esta tarea utilizaremos la herramienta \gls{term:fluentd}.

\gls{term:fluentd} es un recolector de datos multiplataforma y de código abierto
escrito en \gls{term:ruby} y desarrollado por la compañía TreasureData. Esta
herramienta permite unificar el consumo y recolección de datos.

\gls{term:fluentd} cuenta con un sistema flexible de \eng{plugins} que permite
a la comunidad extender su funcionalidad. Hoy en día cuenta con más de 300
\eng{plugins} que permiten tomar datos de docenas de fuentes diferentes.

\gls{term:fluentd} es muy utilizado debido a su mínimo consumo de memoria, su
gran poder de procesamiento de datos, su robustez y su alta disponibilidad.

Además de \gls{term:fluentd} hemos analizado las herramientas
\gls{term:logstash}, \gls{term:heka} y \gls{term:splunk}:

\begin{itemize}

\item
\textbf{\gls{term:logstash}} es un motor de recolección de datos de código abierto.
\gls{term:logstash} puede unificar dinámicamente datos de fuentes dispares y
normalizar los datos en destinos de su elección.

\gls{term:logstash} funciona como un canal de procesamiento de datos del lado
de servidor, que toma datos de una multitud de fuentes de forma simultánea, los
transforma y envía para que sirvan como entrada de otra herramienta.

Con \gls{term:logstash} es posible recolectar \eng{logs} y cualquier tipo de evento.
Estos eventos pueden ser transformados a partir de una amplia gama de entradas,
filtros y \eng{plugins} de salida.

\item
\textbf{\gls{term:heka}} es una solución de manejo de \eng{logs} de código abierto. Fue
diseñada por el equipo de Mozilla y está escrita en el lenguaje de programación
\gls{term:go}. Al igual que \gls{term:logstash}, provee un sistema basado en
\eng{plugins} para recolectar y procesar \eng{logs}. Además permite dirigir las
salidas a una variedad de destinos, incluyendo \gls{term:elasticsearch}.
Lamentablemente, \gls{term:heka} no se encuentra actualmente en mantenimiento.

\item
\textbf{\gls{term:splunk}} es una herramienta comercial para buscar, monitorizar y
analizar datos de aplicaciones y sistemas a través de una interfaz
\eng{web}. Cuenta con soluciones basadas en la nube. \gls{term:splunk}
captura, indexa y correlaciona datos en tiempo real y los almacena en un
repositorio, desde donde se pueden recuperar para generar gráficos, alertas y
tableros de control definibles por un usuario.

\end{itemize}

Hemos descartado el uso de \gls{term:heka}, por no ser mantenida de forma
activa, y de \gls{term:splunk}, por ser \eng{software} privativo.

\gls{term:fluentd} y \gls{term:logstash} resuelven problemas similares y tienen
una fácil integración con \gls{term:docker}. Finalmente hemos elegido
\gls{term:fluentd} para nuestra solución en lugar de \gls{term:logstash},
porque \gls{term:fluentd} se configura usando \gls{term:ruby}, lo que es una
gran ventaja para el laboratorio, ya que sus programadores cuentan con
experiencia en ese lenguaje.

\gls{term:fluentd} usa el formato \gls{acro:json} para estructurar los datos,
lo que permite unificar todas las facetas del procesamiento de datos de \eng{logs}:
recolección, filtrado, \eng{buffering} y envío de \eng{logs} a través de múltiples
fuentes y destinos.

Una vez que se tiene \gls{term:fluentd} instalado, debemos configurarlo para
que pueda leer los datos de los \eng{logs} desde la salida estándar de
\gls{term:docker}, y enviarlos a nuestra instancia de \gls{term:elasticsearch}.

Para lograr este objetivo agregaremos dos \glspl{term:gema} a la configuración de
\gls{term:fluentd} que nos permitirán enviar los datos a
\gls{term:elasticsearch} y \eng{parsear} los datos de distintas fuentes para tomar la
información que creamos importante. Esto lo haremos en un archivo llamado
\texttt{start.sh}:

\begin{lstlisting}

#!/bin/sh

gem install fluent-plugin-elasticsearch
gem install fluent-plugin-parser
exec fluentd -c /fluentd/etc/$FLUENTD_CONF -p /fluentd/plugins $FLUENTD_OPT

\end{lstlisting}

Este archivo instala las \glspl{term:gema} necesarias, y luego inicia el
servicio de \gls{term:fluentd}, definiendo la ruta del archivo de configuración
y la carpeta en la que se encuentran los \eng{plugins}.

La \gls{term:gema} \texttt{fluent-plugin-elasticsearch} permite que \gls{term:fluentd}
envíe sus datos a \gls{term:elasticsearch}. Este plugin crea índices de
\gls{term:elasticsearch} de forma automática. Basta con agregar a la
configuración de \gls{term:fluentd}, el atributo \lstinline{type} con valor
\gls{term:elasticsearch} y de forma opcional, parámetros adicionales. Por
ejemplo:

\begin{lstlisting}

<match my.logs>
  type elasticsearch
  host localhost
  port 9200
  index_name fluentd
  type_name fluentd
</match>

\end{lstlisting}

La \gls{term:gema} \texttt{fluent-plugin-parser} permite a \gls{term:fluentd}
reconocer patrones en las cadenas de texto y remitirlas. Por ejemplo, el
siguiente código es capaz de reconocer a partir de una expresión regular el
\eng{host}, usuario y hora en un mensaje de Apache:

\begin{lstlisting}

<match raw.apache.common.*>
  @type parser
  remove_prefix raw
  format /^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\])$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  key_name message
</match>

\end{lstlisting}

El siguiente paso es escribir la configuración de \gls{term:fluentd}, en el
archivo \texttt{fluentd.conf}:

\begin{lstlisting}

<source>
  type forward
  port 24224
</source>

<match rails.docker.**>
  type parser
  key_name log
  format json
  reserve_data yes
  add_prefix logs
</match>

<match nginx.docker.**>
  type parser
  key_name log
  format /^(?<remote_addr>[^ ]*) - (?<remote_user>[^ ]*) \[(?<time>[^\]]*)\] "(?<request_type>[^ ]*) (?<request_url>[^ ]*) (?<request_http_protocol>[^ ]*)" (?<status_code>[^ ]*) (?<request_size>[^ ]*) "(?<referer>[^\"]*)" "(?<user_agent>[^\"]*)" "(?<unknown>[^\"]*)"?$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  reserve_data yes
  add_prefix logs
</match>

<match logs.rails.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix rails-logs
  flush_interval 3s
</match>

<match logs.nginx.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix nginx-logs
  flush_interval 3s
</match>

\end{lstlisting}

A continuación explicaremos el funcionamiento del código, bloque por bloque.

\begin{lstlisting}

<source>
  type forward
  port 24224
</source>

\end{lstlisting}

Las fuentes de entrada de \gls{term:fluentd} se habilitan seleccionando y
configurando los \eng{plugins} de entrada deseados usando la directiva
\lstinline{source}. La instrucción \lstinline{type forward} convierte a
\gls{term:fluentd} en un \eng{endpoint} \gls{acro:tcp}. La línea
\lstinline{port 24224}, indica el puerto en donde \gls{term:fluentd} aceptará
paquetes \gls{acro:tcp} \footnote{Cf.
\url{http://docs.fluentd.org/v0.12/articles/config-file}}.

\begin{lstlisting}

<match rails.docker.**>
  type parser
  key_name log
  format json
  reserve_data yes
  add_prefix logs
</match>

\end{lstlisting}

La directiva \lstinline{match} busca eventos con etiquetas que coincidan con un
patrón y los procesa. En este caso, sólo tomará las entradas con etiquetas
que comiencen con \lstinline{'rails.docker.'}. Las instrucciones
\lstinline{type parser}, \lstinline{key_name log} y \lstinline{format json}
nos indican que la clave de nombre \lstinline{log} en la entrada será analizada
sintácticamente y transformada a formato \gls{acro:json}.

La línea \lstinline{reserve_data yes} es usada para mantener los datos
originales. Por ejemplo, si la entrada fue la cadena de texto
\texttt{'log': \{"user":1,"num":2\}}, la salida será:

\begin{lstlisting}

{"log":"{\"user\":1,\"num\":2}","log.user":1, "log.num":2}

\end{lstlisting}

Finalmente, la instrucción \lstinline{add_prefix logs} es utilizada para
agregar la subcadena \lstinline{logs.} al comienzo del nombre de la etiqueta.
Por ejemplo, si la etiqueta se llamaba \lstinline{rails.docker.app1}, la nueva
etiqueta será \lstinline{logs.rails.docker.app1}.

De esta forma, la salida de este bloque se podrá convertir en la entrada de
otro bloque.

\begin{lstlisting}

<match nginx.docker.**>
  type parser
  key_name log
  format /^(?<remote_addr>[^ ]*) - (?<remote_user>[^ ]*) \[(?<time>[^\]]*)\] "(?<request_type>[^ ]*) (?<request_url>[^ ]*) (?<request_http_protocol>[^ ]*)" (?<status_code>[^ ]*) (?<request_size>[^ ]*) "(?<referer>[^\"]*)" "(?<user_agent>[^\"]*)" "(?<unknown>[^\"]*)"?$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  reserve_data yes
  add_prefix logs
</match>

\end{lstlisting}

Este bloque funciona de forma similar al anterior. Toma aquellas entradas con
etiquetas que comienzan con la cadena \lstinline{'nginx.docker.''}, es decir,
aquellas que provengan de \gls{term:nginx}.

En lugar de utilizar el formato \gls{acro:json}, se usa una expresión regular
para recuperar los valores importantes de la expresión. Algunos de estos
valores son la marca de tiempo, el tipo de solicitud \eng{web}, la \gls{acro:url} a
la que se intentó acceder, el protocolo \gls{acro:http}, el código de estado
\gls{acro:http}, el tamaño del mensaje de solicitud y el cliente \eng{web}
utilizado.

Las marcas de tiempo suelen ser problemáticas en los \eng{logs}. Cada fuente de
registros puede tener formatos de hora y fecha diferentes, lo que dificulta el
análisis de estos datos. La instrucción \lstinline{time_format} permite
definir el formato que se quiere que cumplan las fechas y de esa forma evitar
estos problemas.

\begin{lstlisting}

<match logs.rails.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix rails-logs
  flush_interval 3s
</match>

<match logs.nginx.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix nginx-logs
  flush_interval 3s
</match>

\end{lstlisting}

Los últimos dos bloques toman como entrada las salidas de los bloques
anteriores, y las envían a \gls{term:elasticsearch} con un formato adecuado,
haciendo uso del \eng{plugin} \texttt{fluentd-plugin-elasticsearch}. Este
\eng{plugin} además, crea índices en \gls{term:elasticsearch}.

Teniendo en consideración la arquitectura general de la infraestructura, hemos
decidido, para nuestra solución, que \gls{term:fluentd} sea ejecutado sobre su
propio \gls{term:contenedor} de \gls{term:docker}. Para lograr que este
\gls{term:contenedor} se comunique con el \gls{term:contenedor} de la aplicación
y con la base de datos, usaremos la herramienta Compose de \gls{term:docker}.

Creamos el archivo \texttt{docker-compose.yml} y agregamos la siguiente
configuración:

\begin{lstlisting}

fluentd:
    image: fluent/fluentd:latest
    ports:
      - "24224:24224"
    volumes:
      - ./fluentd/etc:/fluentd/etc
    command: /fluentd/etc/start.sh
    networks:
      - lognet

\end{lstlisting}

La línea \lstinline{image: fluent/fluentd:latest} indica la imagen de
\gls{term:docker} que se va a utilizar. En este caso, se utiliza la imagen
oficial de \gls{term:fluentd}\footnote{Cf.
\url{https://hub.docker.com/r/fluent/fluentd/l}}.

La clave \lstinline{ports} se utiliza para exponer puertos. Gracias a esta capacidad es
posible conectar el \gls{term:contenedor} con el sistema externo, para intercambiar
información por \eng{sockets}, y de la misma forma, se permite conectar varios
\glspl{term:contenedor} entre sí.

Para usar esta configuración se requiere indicar puerto origen y puerto
destino. El puerto origen es interno al \gls{term:contenedor}, y el puerto destino existe
sólo en el entorno no virtualizado. En el ejemplo, un \gls{term:contenedor} abre el puerto
interno 24224 y lo mapea con el puerto del entorno externo 24224.

La configuración volumes es usada para compartir el directorio
\lstinline{fluentd/etc} con el \gls{term:contenedor}. La instrucción command:
\texttt{/fluentd/etc/start.sh} sirve para que el script que corre
\gls{term:fluentd} sea ejecutado al poner en marcha el \gls{term:contenedor}.

Finalmente, para que los \glspl{term:contenedor} con \gls{term:nginx} y con las aplicaciones rails
pueda enviar los datos a \gls{term:fluentd}, se agrega en el archivo
\lstinline{docker-compose.yml} las siguientes líneas:

\begin{lstlisting}

nginx:
  logging:
    driver: fluentd

app:
  logging:
    driver: fluentd

\end{lstlisting}

Se puede encontrar la configuración completa de ejemplo en el \autoref{anexo:C}.

Tenemos configurada la salida de los \eng{logs} de la
aplicación \gls{term:ror}, la recolección y \eng{parseo} de los mismos con
\gls{term:fluentd} y el envío de los mismos a la base de datos destinada a este
fin con \gls{term:elasticsearch}. Ya es posible consultar a nuestra base
de datos información sobre los \eng{logs}.
