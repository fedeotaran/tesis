\subsection{Unificación y recolección}
\label{unificacion_y_recoleccion}

El siguiente paso es tomar los datos de ambas fuentes, unificarlos y enviarlos
a nuestra instancia en ejecución de Elasticsearch. Para cumplir con esta tarea
utilizaremos la herramienta FluentD.

Fluentd es un colector de datos multiplataforma y de código abierto escrito en
Ruby y desarrollado por la compañía TreasureData. Esta herramienta permite
unificar el consumo y recolección de datos.

Fluentd cuenta con un sistema flexible de plugins que permite a la comunidad
extender su funcionalidad. Hoy en día cuenta con más de 300 plugins que
permiten tomar datos de docenas de fuentes diferentes.

Fluentd es muy utilizado debido a su mínimo consumo de memoria, su gran poder
de procesamiento de datos, su robustez y su alta disponibilidad.

Además de Fluentd hemos analizado las siguientes herramientas para resolver
este problema:

Logstash Heka Splunk

Logstash es un motor de recolección de datos de código abierto. Logstash puede
unificar dinámicamente datos de fuentes dispares y normalizar los datos en
destinos de su elección. 

Logstash funciona como un canal de procesamiento de datos del lado de servidor,
que toma datos de una multitud de fuentes de forma simultánea, los transforma y
envía para que sirvan como entrada de otra herramienta.

Con Logstash es posible recolectar logs y cualquier tipo de evento. Estos
eventos pueden ser transformados a partir de una amplia gama de entradas,
filtros y plugins de salida.  \url{https://www.elastic.co/products/logstash}

Heka es una solución de manejo de logs de código abierto. Fue diseñada por el
equipo de Mozilla y está escrita en el lenguaje de programación Go. Al igual
que Logstash, provee un sistema basado en plugins para recolectar y procesar
logs. Además permite dirigir las salidas a una variedad de destinos, incluyendo
Elasticsearch. Lamentablemente, Heka no se encuentra actualmente en
mantenimiento.

Splunk es una herramienta comercial para buscar, monitorizar y analizar datos
de aplicaciones y sistemas a través de una interfaz web. Cuenta con soluciones
basadas en la nube. Splunk captura, indexa y correlaciona datos en tiempo real
y los almacena en un repositorio, desde donde se pueden recuperar para generar
gráficos, alertas y tableros de control definibles por un usuario.

Hemos descartado el uso de Heka, por no ser mantenida de forma activa, y de
Splunk, por ser software privativo.

Fluentd y Logstash resuelven problemas similares y tienen una fácil integración
con Docker. Finalmente hemos elegido Fluentd para nuestra solución en lugar de
Logstash, porque Fluentd se configura usando Ruby, lo que es una gran ventaja
para el laboratorio, ya que sus programadores cuentan con experiencia en ese
lenguaje.

Fluentd usa el formato JSON para estructurar los datos, lo que permite unificar
todas las facetas del procesamiento de datos de logs: recolección, filtrado,
buffering y envío de logs a través de múltiples fuentes y destinos.

Una vez que se tiene Fluentd instalado, debemos configurarlo para que pueda
leer los datos de los logs desde la salida estándar de docker, y enviarlos a
nuestra instancia de Elasticsearch.

Para lograr este objetivo agregaremos dos gemas a la configuración de fluentd
que nos permitirán enviar los datos a elasticsearch y parsear los datos de
distintas fuentes para tomar la información que creamos importante. Esto lo
haremos en un archivo llamado start.sh:

\begin{lstlisting}

#!/bin/sh

gem install fluent-plugin-elasticsearch
gem install fluent-plugin-parser
exec fluentd -c /fluentd/etc/$FLUENTD_CONF -p /fluentd/plugins $FLUENTD_OPT

\end{lstlisting}

Este archivo instala las gemas que necesarias, y luego inicia el servicio de
fluentd, pasandole la ruta del archivo de configuración y la carpeta en la que
se encuentran los plugins.

La gema fluent-plugin-elasticsearch permite que fluentD pueda enviar sus datos
a Elasticsearch. Este plugin crea índices de Elasticsearch simplemente
escribiendolos. Basta con agregar a la configuración de Fluentd, el atributo
type con valor elasticsearch y opcionalmente configuraciones adicionales. Por
ejemplo: 

\begin{lstlisting}

<match my.logs>
  type elasticsearch
  host localhost
  port 9200
  index_name fluentd
  type_name fluentd
</match>

\end{lstlisting}

La gema fluent-plugin-parser permite a FluentD reconocer patrones en las
cadenas de texto y reemitirlas. Por ejemplo, el siguiente código es capaz de
reconocer a partir de una expresión regular las partes importantes del mensaje
(host, user y time):

\begin{lstlisting}

<match raw.apache.common.*>
  @type parser
  remove_prefix raw
  format /^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\])$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  key_name message
</match>

\end{lstlisting}

El siguiente paso es escribir la configuración de FluentD, en el archivo
fluentd.conf:

\begin{lstlisting}

<source>
  type forward
  port 24224
</source>

<match rails.docker.**>
  type parser
  key_name log
  format json
  reserve_data yes
  add_prefix logs
</match>

<match nginx.docker.**>
  type parser
  key_name log
  format /^(?<remote_addr>[^ ]*) - (?<remote_user>[^ ]*) \[(?<time>[^\]]*)\] "(?<request_type>[^ ]*) (?<request_url>[^ ]*) (?<request_http_protocol>[^ ]*)" (?<status_code>[^ ]*) (?<request_size>[^ ]*) "(?<referer>[^\"]*)" "(?<user_agent>[^\"]*)" "(?<unknown>[^\"]*)"?$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  reserve_data yes
  add_prefix logs
</match>

<match logs.rails.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix rails-logs
  flush_interval 3s
</match>

<match logs.nginx.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix nginx-logs
  flush_interval 3s
</match>

\end{lstlisting}

A continuación explicaremos el funcionamiento del código, bloque por bloque.

\begin{lstlisting}

<source>
  type forward
  port 24224
</source>

\end{lstlisting}

Las fuentes de entrada de Fluentd se habilitan seleccionando y configurando los
plugins de entrada deseados usando la directiva source. La instrucción type
forward convierte a fluentd en un endpoint TCP. La línea port 24224, indica el
puerto en donde Fluentd aceptará paquetes TCP.
\url{http://docs.fluentd.org/v0.12/articles/config-file}

\begin{lstlisting}

<match rails.docker.**>
  type parser
  key_name log
  format json
  reserve_data yes
  add_prefix logs
</match>

\end{lstlisting}

La directiva \lstinline{match} busca eventos con etiquetas que coincidan con un
patrón y los procesa. En este caso, sólo tomará las entradas con etiqueta
comenzando con \lstinline{rails.docker.}. Las instrucciones type parser,
\lstinline{key\_name} log y format json nos indican que la clave de nombre
‘log’ en la entrada será analizada sintácticamente y transformada a formato
JSON.

La línea \lstinline{reserve\_data} yes es usada para mantener los datos
originales. Por ejemplo, si la entrada fue la cadena de texto
\texttt{'log': \{"user":1,"num":2\}}, la salida será:

\begin{lstlisting}

{"log":"{\"user\":1,\"num\":2}","log.user":1, "log.num":2}

\end{lstlisting}

Finalmente, la instrucción add\_prefix logs es utilizada para agregar la
subcadena “logs.” al comienzo del nombre de la etiqueta. Por ejemplo, si la
etiqueta se llamaba rails.docker.app1, la nueva etiqueta será
\lstinline{logs.rails.docker.app1}.

De esta forma, la salida de este bloque se podrá convertir en la entrada de
otro bloque.

\begin{lstlisting}

<match nginx.docker.**>
  type parser
  key_name log
  format /^(?<remote_addr>[^ ]*) - (?<remote_user>[^ ]*) \[(?<time>[^\]]*)\] "(?<request_type>[^ ]*) (?<request_url>[^ ]*) (?<request_http_protocol>[^ ]*)" (?<status_code>[^ ]*) (?<request_size>[^ ]*) "(?<referer>[^\"]*)" "(?<user_agent>[^\"]*)" "(?<unknown>[^\"]*)"?$/
  time_format \%d/\%b/\%Y:\%H:\%M:\%S \%z
  reserve_data yes
  add_prefix logs
</match>

\end{lstlisting}

Este bloque funciona de forma similar al anterior. Toma aquellas entradas con
etiquetas que comienzan con la cadena \lstinline{nginx.docker.}, es decir, aquellas que
provengan de NGINX.

En lugar de utilizar el formato JSON, se usa una expresión regular para
recuperar los valores importantes de la expresión. Algunos de estos valores son
la marca de tiempo, el tipo de solicitud web, la URL a la que se intentó
acceder, el protocolo HTTP, el código de estado HTTP, el tamaño del mensaje de
solicitud y el cliente web utilizado.

Las marcas de tiempo suelen ser problemáticas en los logs. Cada fuente de
registros puede tener formatos de hora y fecha diferentes, lo que dificulta el
análisis de estos datos. La instrucción \lstinline{time\_format} permite definir el formato
que se quiere que cumplan las fechas y de esa forma evitar estos problemas.

\begin{lstlisting}
 
<match logs.rails.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix rails-logs
  flush_interval 3s
</match>

<match logs.nginx.**>
  type elasticsearch
  host elasticsearch
  logstash_format true
  logstash_prefix nginx-logs
  flush_interval 3s
</match>

\end{lstlisting}

Los últimos dos bloques toman como entrada las salidas de los bloques
anteriores, y las envían a Elasticsearch con un formato adecuado, haciendo uso
del plugin fluentd-plugin-elasticsearch. Este plugin además, crea índices en
Elasticsearch.

Teniendo en consideración la arquitectura general de la infraestructura, hemos
decidido, para nuestra solución, que Fluentd sea ejecutado sobre su propio
contenedor de Docker. Para lograr que este contenedor se comunique con el
contenedor de la aplicación y con la base de datos, usaremos la herramienta
Compose de Docker.

Se puede encontrar una descripción más completa de Compose en el Anexo 3 [REF].

Creamos el archivo \texttt{docker-compose.yml} y agregamos la siguiente configuración:

\begin{lstlisting}

fluentd:
    image: fluent/fluentd:latest
    ports:
      - "24224:24224"
    volumes:
      - ./fluentd/etc:/fluentd/etc
    command: /fluentd/etc/start.sh
    networks:
      - lognet

\end{lstlisting}

La línea image: fluent/fluentd;latest indica la imagen de docker que se va a
utilizar. En este caso, se utiliza la imagen oficial de Fluentd. (pie de
página: \url{https://hub.docker.com/r/fluent/fluentd/l}).

La clave ports se utiliza para exponer puertos. Gracias a esta capacidad es
posible conectar el contenedor con el sistema externo, para intercambiar
información por sockets, y de la misma forma, se permite conectar varios
contenedores entre sí.

Para usar esta configuración se requiere indicar puerto origen y puerto
destino. El puerto origen es interno al contenedor, y el puerto destino existe
sólo en el entorno no virtualizado. En el ejemplo, un contenedor abre el puerto
interno 24224 y lo mapea con el puerto del entorno externo 24224.

La configuración volumes es usada para compartir el directorio .fluentd/etc con
el contenedor. La instrucción command: \texttt{/fluentd/etc/start.sh} sirve para que el
script que corre Fluentd sea ejecutado al poner en marcha el contenedor.

Finalmente, para que los contenedores con NGINX y con las aplicaciones rails
pueda enviar los datos a Fluentd, se agrega en el archivo docker-compose las
siguientes líneas:

\begin{lstlisting}

nginx:
  logging:
    driver: fluentd

app:
  logging:
    driver: fluentd

\end{lstlisting}

Se puede encontrar la configuración completa de ejemplo en el Anexo 4 [REF].

En esta instancia ya tenemos configurada la salida de los logs de la aplicación
de la aplicación rails, la recolección y parseo de los mismos con fluentD y el
envío de los mismos a la base de datos destinada a este fin con ElasticSearch.
Ahora ya es posible consultar en nuestra base de datos información sobre los
logs.

